import { PageLayout } from '../src/components/PageLayout';

<PageLayout title="Generative Testing" description="Generative testing strategy for the DyGram machine language parser and transformation pipeline">

## Overview

This document outlines the generative testing strategy for the DyGram machine language parser and transformation pipeline.

## Philosophy

Traditional static test suites validate known patterns but often miss edge cases. Generative testing creates diverse test cases programmatically to explore a wider input space and stress-test the system with unexpected combinations.

## Architecture

### Test Generation
- **MachineGenerator class**: Programmatically creates valid DyGram machines with varying complexity
- **Randomized parameters**: Node counts, nesting depths, edge patterns selected dynamically
- **Diverse patterns**: Unicode, special characters, deep nesting, large graphs, complex attributes

### Validation Pipeline

Each generated machine flows through a complete validation pipeline:

```
DyGram Source → Parser → AST → JSON → Mermaid → Validation
```

**Validation stages:**

1. **Parse validation**: Checks for lexer/parser errors
2. **Completeness**: Verifies all source elements captured in AST
3. **JSON transformation**: Validates serialization to JSON format
4. **Mermaid generation**: Validates diagram generation
5. **Structural validation**: Verifies nodes/edges present in Mermaid output
6. **Losslessness**: Verifies transformations preserve information

**Note on Mermaid parsing validation**: Mermaid.js requires a browser environment (DOM APIs) for full parsing validation. In the Node.js test environment, we perform structural validation instead. For comprehensive Mermaid validation, manually review the generated artifacts in `test-output/generative/` - each file contains the source, JSON, and rendered Mermaid for visual inspection.

### Output Artifacts

Test runs generate inspection artifacts in `test-output/generative/`:
- Individual markdown files per test (source, JSON, Mermaid)
- Comprehensive `REPORT.md` with validation results
- Human-readable format for manual review

## Test Categories

### Basic Validation
- Minimal machines (single node)
- Simple node collections
- Typed nodes (task, state, init, context)

### Structural Complexity
- Deep nesting (2-5+ levels)
- Large machines (50-200+ nodes)
- Complex parent-child relationships

### Edge Features
- Unicode characters in names and labels
- Special characters and quoted identifiers
- All arrow types (→, -->, =>, `<-->`)
- Edge labels with attributes

### Attribute Testing
- Typed attributes (string, number, boolean, arrays)
- Deep attribute structures
- Long text values
- Mixed attribute combinations

### Edge Cases
- Empty machines
- Single-character identifiers
- Multiple edges from one node
- Circular references
- Context-heavy machines

### Stress Testing
- 10 randomized machines per run
- Random parameters (node count, depth, edges)
- Validates robustness across input space

## Running Tests

```bash
# Run all tests including generative suite
npm test

# Run only generative tests
npm test -- test/integration/generative.test.ts
```

## Interpreting Results

### Test Report
Check `test-output/generative/REPORT.md` for:
- Success rate
- Issue counts by category
- Failed test details

### Individual Test Output
Each test generates `test-output/generative/<test_name>.md` with:
- Source DyGram code
- Generated JSON
- Generated Mermaid diagram
- Validation status

### Common Issues

**Parse errors**: Grammar doesn't handle input pattern
**Completeness issues**: AST missing source elements
**Transform errors**: JSON/Mermaid generation failed
**Losslessness issues**: Information lost in transformation
**Mermaid parse errors**: Generated Mermaid invalid

## Extending Tests

### Adding Test Generators

Add new generator methods to `MachineGenerator` class:

```typescript
generateMyPattern(): string {
    return `machine "My Test Pattern"
    // ... your pattern
    `;
}
```

### Adding Test Cases

Add test case in the test suite:

```typescript
test('Generated: My Pattern', async () => {
    const result = await runGenerativeTest(
        'My Pattern',
        () => generator.generateMyPattern()
    );
    expect(result.passed).toBe(true);
});
```

## Benefits of Generative Testing

1. **Bug discovery**: Found 2 critical bugs that 109 static tests missed
2. **Coverage**: Tests input combinations not manually anticipated
3. **Confidence**: Validates robustness across diverse inputs
4. **Documentation**: Test generators serve as executable specifications
5. **Regression prevention**: Catches bugs from code changes
6. **Visual validation**: Generates artifacts for human review

## Best Practices

- **Start simple**: Begin with basic patterns, add complexity incrementally
- **Document patterns**: Name tests descriptively
- **Review artifacts**: Manually inspect generated outputs periodically
- **Balance coverage**: Mix targeted patterns with randomized tests
- **Track failures**: Investigate and fix all failures immediately
- **Expand continuously**: Add generators when bugs found or features added

## Future Enhancements

- **Property-based testing**: Use hypothesis/fast-check for deeper generation
- **Mutation testing**: Automatically create invalid inputs
- **Performance benchmarking**: Track generation/parsing performance
- **Fuzzing integration**: Connect with fuzzers for security testing
- **Visual diff tool**: Compare Mermaid outputs graphically
- **CI integration**: Run subset of generative tests on every commit

</PageLayout>
