{
  "json": {
    "title": "LLM Processing Pipeline",
    "attributes": [],
    "annotations": [],
    "nodes": [
      {
        "name": "llmConfig",
        "type": "context",
        "attributes": [
          {
            "name": "model",
            "value": "claude-3-5-sonnet-20241022"
          },
          {
            "name": "temperature",
            "type": "Float",
            "value": 0.7
          },
          {
            "name": "maxTokens",
            "type": "Integer",
            "value": 4096
          }
        ]
      },
      {
        "name": "userInput",
        "type": "context",
        "attributes": [
          {
            "name": "question",
            "value": "#userQuestion"
          },
          {
            "name": "context",
            "value": "#conversationHistory"
          }
        ]
      },
      {
        "name": "analyze",
        "type": "task",
        "attributes": [
          {
            "name": "model",
            "value": "{{ llmConfig.model }}"
          },
          {
            "name": "temperature",
            "value": "{{ llmConfig.temperature }}"
          },
          {
            "name": "prompt",
            "value": "Analyze this question: {{ userInput.question }}\nContext: {{ userInput.context }}"
          }
        ]
      },
      {
        "name": "enhance",
        "type": "task",
        "attributes": [
          {
            "name": "model",
            "value": "{{ llmConfig.model }}"
          },
          {
            "name": "prompt",
            "value": "Enhance the analysis: {{ analyze.prompt }}"
          }
        ]
      },
      {
        "name": "format",
        "type": "task",
        "attributes": [
          {
            "name": "prompt",
            "value": "Format the final response based on: {{ enhance.prompt }}"
          }
        ]
      }
    ],
    "edges": [
      {
        "source": "analyze",
        "target": "enhance",
        "arrowType": "->"
      },
      {
        "source": "enhance",
        "target": "format",
        "arrowType": "->"
      }
    ],
    "inferredDependencies": [
      {
        "source": "analyze",
        "target": "llmConfig",
        "reason": "reads model",
        "path": "llmConfig.model"
      },
      {
        "source": "analyze",
        "target": "llmConfig",
        "reason": "reads temperature",
        "path": "llmConfig.temperature"
      },
      {
        "source": "analyze",
        "target": "userInput",
        "reason": "reads prompt",
        "path": "userInput.question"
      },
      {
        "source": "analyze",
        "target": "userInput",
        "reason": "reads prompt",
        "path": "userInput.context"
      },
      {
        "source": "enhance",
        "target": "llmConfig",
        "reason": "reads model",
        "path": "llmConfig.model"
      },
      {
        "source": "enhance",
        "target": "analyze",
        "reason": "reads prompt",
        "path": "analyze.prompt"
      },
      {
        "source": "format",
        "target": "enhance",
        "reason": "reads prompt",
        "path": "enhance.prompt"
      }
    ]
  },
  "graphviz": "digraph {\n  // Graph attributes\n  label=<<table border=\"0\" cellborder=\"0\" cellspacing=\"0\" cellpadding=\"4\"><tr><td align=\"center\"><font point-size=\"12\"><b>LLM Processing Pipeline</b></font></td></tr></table>>;\n  labelloc=\"t\";\n  fontsize=10;\n  fontname=\"Arial\";\n  compound=true;\n  rankdir=TB;\n  pad=0.25;\n  node [fontname=\"Arial\", fontsize=10];\n  edge [fontname=\"Arial\", fontsize=9];\n\n  // Node definitions with nested namespaces\n  \"llmConfig\" [label=<<table border=\"0\" cellborder=\"0\" cellspacing=\"0\" cellpadding=\"4\"><tr><td align=\"left\"><b>llmConfig</b> <i>&lt;context&gt;</i></td></tr><tr><td><table border=\"0\" cellborder=\"1\" cellspacing=\"0\" cellpadding=\"2\" align=\"left\"><tr><td port=\"model__key\" align=\"left\" balign=\"left\">model</td><td port=\"model__value\" align=\"left\" balign=\"left\">claude-3-5-sonnet-20241022</td></tr><tr><td port=\"temperature__key\" align=\"left\" balign=\"left\">temperature : Float</td><td port=\"temperature__value\" align=\"left\" balign=\"left\">0.7</td></tr><tr><td port=\"maxTokens__key\" align=\"left\" balign=\"left\">maxTokens : Integer</td><td port=\"maxTokens__value\" align=\"left\" balign=\"left\">4096</td></tr></table></td></tr></table>>, pad=0.5, shape=folder, fillcolor=\"#E8F5E9\", style=filled, color=\"#388E3C\"];\n  \"userInput\" [label=<<table border=\"0\" cellborder=\"0\" cellspacing=\"0\" cellpadding=\"4\"><tr><td align=\"left\"><b>userInput</b> <i>&lt;context&gt;</i></td></tr><tr><td><table border=\"0\" cellborder=\"1\" cellspacing=\"0\" cellpadding=\"2\" align=\"left\"><tr><td port=\"question__key\" align=\"left\" balign=\"left\">question</td><td port=\"question__value\" align=\"left\" balign=\"left\">#userQuestion</td></tr><tr><td port=\"context__key\" align=\"left\" balign=\"left\">context</td><td port=\"context__value\" align=\"left\" balign=\"left\">#conversationHistory</td></tr></table></td></tr></table>>, pad=0.5, shape=folder, fillcolor=\"#E8F5E9\", style=filled, color=\"#388E3C\"];\n  \"analyze\" [label=<<table border=\"0\" cellborder=\"0\" cellspacing=\"0\" cellpadding=\"4\"><tr><td align=\"left\"><b>analyze</b> <i>&lt;task&gt;</i></td></tr><tr><td align=\"left\">Analyze this question: {{<br/>userInput.question }}\nContext: {{<br/>userInput.context }}</td></tr><tr><td><table border=\"0\" cellborder=\"1\" cellspacing=\"0\" cellpadding=\"2\" align=\"left\"><tr><td port=\"model__key\" align=\"left\" balign=\"left\">model</td><td port=\"model__value\" align=\"left\" balign=\"left\">{{ llmConfig.model }}</td></tr><tr><td port=\"temperature__key\" align=\"left\" balign=\"left\">temperature</td><td port=\"temperature__value\" align=\"left\" balign=\"left\">{{ llmConfig.temperature }}</td></tr></table></td></tr></table>>, pad=0.5, shape=box, fillcolor=\"#E3F2FD\", style=filled, color=\"#1976D2\"];\n  \"enhance\" [label=<<table border=\"0\" cellborder=\"0\" cellspacing=\"0\" cellpadding=\"4\"><tr><td align=\"left\"><b>enhance</b> <i>&lt;task&gt;</i></td></tr><tr><td align=\"left\">Enhance the analysis: {{ analyze.prompt<br/>}}</td></tr><tr><td><table border=\"0\" cellborder=\"1\" cellspacing=\"0\" cellpadding=\"2\" align=\"left\"><tr><td port=\"model__key\" align=\"left\" balign=\"left\">model</td><td port=\"model__value\" align=\"left\" balign=\"left\">{{ llmConfig.model }}</td></tr></table></td></tr></table>>, pad=0.5, shape=box, fillcolor=\"#E3F2FD\", style=filled, color=\"#1976D2\"];\n  \"format\" [label=<<table border=\"0\" cellborder=\"0\" cellspacing=\"0\" cellpadding=\"4\"><tr><td align=\"left\"><b>format</b> <i>&lt;task&gt;</i></td></tr><tr><td align=\"left\">Format the final response based on: {{<br/>enhance.prompt }}</td></tr></table>>, pad=0.5, shape=box, fillcolor=\"#E3F2FD\", style=filled, color=\"#1976D2\"];\n\n  // Edges\n  \"analyze\" -> \"enhance\" [labelOverlay=\"75%\", labelhref=\"#srcLineTBD\"];\n  \"enhance\" -> \"format\" [labelOverlay=\"75%\", labelhref=\"#srcLineTBD\"];\n\n  // Inferred Dependencies\n  \"analyze\" -> \"llmConfig\" [label=\"reads model\", style=dashed, color=blue];\n  \"analyze\" -> \"llmConfig\" [label=\"reads temperature\", style=dashed, color=blue];\n  \"analyze\" -> \"userInput\" [label=\"reads prompt\", style=dashed, color=blue];\n  \"analyze\" -> \"userInput\" [label=\"reads prompt\", style=dashed, color=blue];\n  \"enhance\" -> \"llmConfig\" [label=\"reads model\", style=dashed, color=blue];\n  \"enhance\" -> \"analyze\" [label=\"reads prompt\", style=dashed, color=blue];\n  \"format\" -> \"enhance\" [label=\"reads prompt\", style=dashed, color=blue];\n}",
  "svgHash": "8b697f40a4c325df02069cd7bda90d2fe1604830dc61a09803d5238cd4b70a87"
}